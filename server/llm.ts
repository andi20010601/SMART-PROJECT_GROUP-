/**
 * Standalone LLM Integration
 * * This module provides a unified interface for LLM calls that works with:
 * - OpenAI API (default)
 * - Azure OpenAI
 * - Any OpenAI-compatible API (e.g., Ollama, LocalAI, etc.)
 */

export interface Message {
  role: "system" | "user" | "assistant";
  content: string;
}

export interface LLMOptions {
  messages: Message[];
  temperature?: number;
  maxTokens?: number;
  responseFormat?: {
    type: "json_schema";
    json_schema: {
      name: string;
      strict: boolean;
      schema: Record<string, any>;
    };
  };
}

export interface LLMResponse {
  choices: Array<{
    message: {
      content: string | null;
      role: string;
    };
    finish_reason: string;
  }>;
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

// Get configuration from environment
function getConfig() {
  return {
    apiKey: process.env.LLM_API_KEY || process.env.OPENAI_API_KEY || "",
    apiUrl: process.env.LLM_API_URL || "https://api.openai.com/v1/chat/completions",
    model: process.env.LLM_MODEL || "gpt-4o-mini",
  };
}

/**
 * Invoke LLM with messages
 * Works with OpenAI API and compatible endpoints
 */
export async function invokeLLM(options: LLMOptions): Promise<LLMResponse> {
  const config = getConfig();

  // ============================================================
  // ğŸ”ã€è°ƒè¯•æ—¥å¿—ã€‘è¿™é‡Œæ˜¯æˆ‘æ–°å¢çš„ä»£ç ï¼Œå¸®ä½ ç›‘æ§ AI è°ƒç”¨æƒ…å†µ
  // ============================================================
  console.log("--------------------------------------------------");
  console.log("ğŸ¤– [LLM] æ­£åœ¨å‘èµ· AI è¯·æ±‚...");
  console.log("   ğŸ‘‰ ç›®æ ‡ URL:", config.apiUrl);
  console.log("   ğŸ‘‰ ä½¿ç”¨æ¨¡å‹:", config.model);
  console.log("   ğŸ‘‰ API Key:", config.apiKey ? `âœ… å·²åŠ è½½ (å°¾å·: ${config.apiKey.slice(-4)})` : "âŒ æœªæ‰¾åˆ° Key!");
  console.log("--------------------------------------------------");

  if (!config.apiKey) {
    console.warn("[LLM] âŒ ä¸¥é‡é”™è¯¯: æ²¡æœ‰æ‰¾åˆ° API Keyï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶");
    // Return a mock response for development
    return {
      choices: [{
        message: {
          content: JSON.stringify({
            summary: "ç³»ç»Ÿæœªæ£€æµ‹åˆ° API Keyï¼Œè¿™æ˜¯æ¨¡æ‹Ÿçš„å›å¤ã€‚",
            recommendations: ["è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® LLM_API_KEY"],
          }),
          role: "assistant",
        },
        finish_reason: "stop",
      }],
    };
  }

  const requestBody: Record<string, any> = {
    model: config.model,
    messages: options.messages,
    temperature: options.temperature ?? 0.7,
  };

  if (options.maxTokens) {
    requestBody.max_tokens = options.maxTokens;
  }

  if (options.responseFormat) {
    requestBody.response_format = options.responseFormat;
  }

  try {
    const response = await fetch(config.apiUrl, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "Authorization": `Bearer ${config.apiKey}`,
      },
      body: JSON.stringify(requestBody),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error(`[LLM] âŒ è¯·æ±‚è¢«æ‹’ç»! çŠ¶æ€ç : ${response.status}`);
      console.error(`[LLM] é”™è¯¯è¯¦æƒ…: ${errorText}`);
      throw new Error(`LLM API error: ${response.status} - ${errorText}`);
    }

    const data = await response.json();
    console.log("âœ… [LLM] è¯·æ±‚æˆåŠŸ! AI å·²å›å¤ã€‚");
    return data as LLMResponse;

  } catch (error) {
    console.error("[LLM] âŒ ç½‘ç»œæˆ–è¯·æ±‚å‘ç”Ÿå¼‚å¸¸:", error);
    throw error;
  }
}

/**
 * Simple text completion helper
 */
export async function generateText(prompt: string, systemPrompt?: string): Promise<string> {
  const messages: Message[] = [];

  if (systemPrompt) {
    messages.push({ role: "system", content: systemPrompt });
  }
  messages.push({ role: "user", content: prompt });

  const response = await invokeLLM({ messages });
  return response.choices[0]?.message?.content || "";
}

/**
 * JSON generation helper with schema validation
 */
export async function generateJSON<T>(
  prompt: string,
  schema: Record<string, any>,
  systemPrompt?: string
): Promise<T> {
  const messages: Message[] = [];

  if (systemPrompt) {
    messages.push({ role: "system", content: systemPrompt });
  }
  messages.push({ role: "user", content: prompt });

  const response = await invokeLLM({
    messages,
    responseFormat: {
      type: "json_schema",
      json_schema: {
        name: "response",
        strict: true,
        schema,
      },
    },
  });

  const content = response.choices[0]?.message?.content;
  if (!content) {
    throw new Error("No content in LLM response");
  }

  return JSON.parse(content) as T;
}